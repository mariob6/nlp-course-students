{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's Problem\n",
    "#### The first problem of today is a classic in ML: sentiment analysis.\n",
    "#### First thing to do is to load and look at the dataset, we use some support functions that you can find in utils/data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFolder = '../data/sentiment/train/'\n",
    "validFolder = '../data/sentiment/valid/'\n",
    "testFolder = '../data/sentiment/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent food .\n",
      "\n",
      "superb customer service .\n",
      "\n",
      "they also have daily specials and ice cream which is really good .\n",
      "\n",
      "it 's a good toasted hoagie .\n",
      "\n",
      "the staff is friendly .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(trainFolder + 'positive.txt', 'r') as fp:\n",
    "    positives = fp.readlines()[:5]\n",
    "    \n",
    "print(\"\\n\".join(positives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... and some negative ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was sadly mistaken .\n",
      "\n",
      "so on to the hoagies , the italian is general run of the mill .\n",
      "\n",
      "minimal meat and a ton of shredded lettuce .\n",
      "\n",
      "nothing really special & not worthy of the $ _num_ price tag .\n",
      "\n",
      "second , the steak hoagie , it is atrocious .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(trainFolder + 'negative.txt', 'r') as fp:\n",
    "    negatives = fp.readlines()[:5]\n",
    "    \n",
    "print(\"\\n\".join(negatives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfortunately, words are not the right representation to carry out Machine Leraning.\n",
    "There are many possible ways to look at sentences in order to let a computer do some maths on it, we start from the classical ones:\n",
    "\n",
    "One-Hot encoding represents each word as a vector $[0, 0, \\dots 0, 1, \\dots, 0]$ of all zeros except of a one in j-th position, and to each word is associated a unique position.\n",
    "\n",
    "This is the basic representation (Vector Space) that most ML algorithms use, it has some drawbacks, the first one is that it requires a lot of memory to store the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luckily, we have already implemented some of these functionalities for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import Corpus\n",
    "corpus = Corpus(trainFolder, validFolder, testFolder, limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0,  1,  2,  3]),\n",
       " tensor([ 4,  5,  6,  2,  3]),\n",
       " tensor([  7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,  18,\n",
       "           2,   3]),\n",
       " tensor([ 19,  20,  21,  18,  22,  23,   2,   3]),\n",
       " tensor([ 24,  25,  16,  26,   2,   3])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train_positive[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'excellent'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.dictionary.idx2word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this training set, a sentence is represented by a sequence of indices.\n",
    "This is not the final version of the training set yet, as our algorithms do not understand sequences (yet), instead each input must be a vector of features.\n",
    "We can convert a sequence of indices to a vector of features by using one-hot encoding, in particular a sentence can be represented by the sum of its one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numDistincWords = len(corpus.dictionary.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def indicesToFeatures(seq, vecLen):\n",
    "    ###\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  1.,  1.,  ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicesToFeatures(corpus.train_positive[0], numDistincWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for some machine learning!\n",
    "We're going to go back to the roots, some good old logistic regression\n",
    "... with some Deep Learning flavour!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a powerful DL package called pyTorch https://pytorch.org/, it has a wonderful community and really easy sintax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the following model:\n",
    "\n",
    "$logit(p) = \\beta_0 + \\beta_1 x_1 + ... \\beta_n x_n$\n",
    "\n",
    "or in a DL syntax:\n",
    "\n",
    "$y = logSoftmax(Ax + b)$\n",
    "\n",
    "The problem is now to find the right coefficients (A, b) in order to \"fit\" the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/Logistic_Classifier.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basics of DL: an optimization problem\n",
    "in order to pose & solve an optimization problem, we need the following ingredients\n",
    "- an objective function\n",
    "- some parameters against which minimize (or maximize) the objective function\n",
    "- an algorithm to optimize\n",
    "\n",
    "In this setting we have\n",
    "- objective function: log likelihood ($ll$)\n",
    "- parameters: (A, b)\n",
    "- algorithm: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "$\\nabla ll = \\sum x_i (y_i - \\hat y _i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent is nice, but...\n",
    "Requires a lot of memory (need to store all the datapoints) and evaluate the model at all the datapoints on each update of the parameters.\n",
    "We can relax this and use at each step only a batch (or so called minibatch) of data.\n",
    "### --> This is the Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/sgd.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LRClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(LRClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  1.,  1.,  ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicesToFeatures(corpus.train_positive[0], numDistincWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.6833, -0.7031])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LRClassifier(2, numDistincWords)\n",
    "clf(indicesToFeatures(corpus.train_positive[0], numDistincWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nice error! \n",
    "This is due to the fact that Torch functions talk only to Torch tensors.\n",
    "Luckily, we can pass from numpy's ndarrays to torch tensors smoothly, just by calling torch.LongTensor(a)\n",
    "\n",
    "Go back and modify the indicesToFeatures function in order to return torch longvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what happens if instead we pass a minibatch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMiniBatch(positives, negatives, batch_size, batch_num, vecLen):\n",
    "    data = positives[batch_num: batch_num + batch_size // 2]\n",
    "    data.extend(negatives[batch_num: batch_num + batch_size // 2])\n",
    "    data = list(map(lambda x: indicesToFeatures(x, vecLen), data))\n",
    "    data = torch.stack(data)\n",
    "    labels = [1] * (batch_size // 2) + [0] * (batch_size // 2)\n",
    "    labels = torch.tensor(labels)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = createMiniBatch(\n",
    "    corpus.train_positive, corpus.train_negative,\n",
    "    12, 0, numDistincWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6833, -0.7031],\n",
       "        [-0.6710, -0.7158],\n",
       "        [-0.6724, -0.7143],\n",
       "        [-0.6758, -0.7108],\n",
       "        [-0.6837, -0.7027],\n",
       "        [-0.6913, -0.6950],\n",
       "        [-0.6768, -0.7098],\n",
       "        [-0.6837, -0.7026],\n",
       "        [-0.6786, -0.7079],\n",
       "        [-0.6682, -0.7188],\n",
       "        [-0.6758, -0.7108],\n",
       "        [-0.6855, -0.7009]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're still missing out the optimization part\n",
    "- Option 1: write by hand the partial derivatives and the update schema of the weights.\n",
    "- Option 2: use pyTorch automatic differentiation's toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/automatic_differentiation.png\" height=\"250\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we need:\n",
    "- loss function: provided by pyTorch torch.nn package, in this case nn.NLLLoss() (standard negative log-likelihood)\n",
    "- optimizer: provided by torch.optim, in this case optim.SGD\n",
    "- a way to differentiate, this is the best part! just call the method backward() on the loss and it will propagate the gradients up to the input data\n",
    "- an update rule: just call optimizer.setp() after the loss has been backpropagated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 20, loss:0.68277 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 280, loss:0.62338 "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(clf.parameters(), lr=0.01)\n",
    "batchSize = 64\n",
    "numBatches = 300\n",
    "\n",
    "for batchNum in range(numBatches):\n",
    "    # Step 1 PyTorch accumulates gradients.\n",
    "    # We need to clear them out before each instance\n",
    "    clf.zero_grad()\n",
    "\n",
    "    # Step 2. Get the datapoints and labels\n",
    "    dataBatch, labelsBatch = createMiniBatch(\n",
    "        corpus.train_positive, corpus.train_negative, batchSize, batchNum, numDistincWords)\n",
    "\n",
    "    # Step 3. Run our forward pass.\n",
    "    logProbs = clf(dataBatch)\n",
    "\n",
    "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "    # calling optimizer.step()\n",
    "    loss = loss_function(logProbs, labelsBatch)\n",
    "    if batchNum % 20 == 0:\n",
    "        print('\\rBatch: {0}, loss:{1:.5f}'.format(batchNum, loss), flush=True, end=\" \")\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is what is called an epoch\n",
    "Usually, we run the same for many epochs\n",
    "\n",
    "TODO: try to improve the results by running several epochs\n",
    "does shuffling the data across epochs improve the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(epochNum, model, loss_function, optimizer, cuda=False):\n",
    "    for batchNum in range(numBatches):\n",
    "        # Step 1 PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        # your code\n",
    "\n",
    "        # Step 2. Get the datapoints and labels\n",
    "        # your code\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        # your code\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(logProbs, labelsBatch)\n",
    "        if batchNum % 20 == 0:\n",
    "            print('\\rEpoch: {0}, Batch: {1}, loss:{2:.5f}'.format(epochNum, batchNum, loss), flush=True, end=\" \")\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, loss:0.60953 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Batch: 280, loss:0.37426 "
     ]
    }
   ],
   "source": [
    "numEpochs = 10\n",
    "\n",
    "def trainModel(numEpochs, model, cuda=False):\n",
    "    # your code\n",
    "        \n",
    "trainModel(numEpochs, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to stop???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can imagine that at each epoch a \"new\" model is trained, so we need an evaluation set to compare\n",
    "\n",
    "We already stored that in corpus.valid.positive & corpus.valid.negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numValidBatches = 300\n",
    "\n",
    "def validate(clf, loss_function, cuda=False):\n",
    "    clf.eval() # turn off gradient propagation\n",
    "    totalLoss = 0\n",
    "    for batchNum in range(numValidBatches):\n",
    "        dataBatch, labelsBatch = createMiniBatch(\n",
    "            corpus.valid_positive, corpus.valid_negative, batchSize, batchNum, numDistincWords)\n",
    "        if cuda:\n",
    "            dataBatch = dataBatch.cuda()\n",
    "            labelsBatch = labelsBatch.cuda()\n",
    "        # Step 3. Run our forward pass.\n",
    "        logProbs = clf(dataBatch)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        totalLoss += loss_function(logProbs, labelsBatch)\n",
    "    return totalLoss / numValidBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6251)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(clf, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/we-need-to-go-deeper.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've seen so fare is an example of a shallow feed-forward neural network.\n",
    "In this context, a layer is a pass of matrix multiplication + nonlinearity --> logSoftmax(Ax + b) is a layer.\n",
    "Deep Neural networks are just sequences of layers of this kind, with different sizes and activation functions.\n",
    "most famous activation functions are:\n",
    "- tanh\n",
    "- softmax\n",
    "- relu\n",
    "- leaky relu\n",
    "And many more to be found here: https://pytorch.org/docs/stable/nn.html?highlight=activation#relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code to get a 3 layer neural network with relu activation functions and final softmax layer to perform binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.firstLayer = nn.Linear(input_dim, 200)\n",
    "        self.secondLayer = nn.Linear(200, 100)\n",
    "        self.thirdLayer = nn.Linear(100, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "       # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have cuda installed, we should use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useCuda = torch.cuda.is_available()\n",
    "useCuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6721, -0.7146],\n",
       "        [-0.6711, -0.7157],\n",
       "        [-0.6663, -0.7208],\n",
       "        [-0.6716, -0.7152],\n",
       "        [-0.6715, -0.7153],\n",
       "        [-0.6717, -0.7151],\n",
       "        [-0.6729, -0.7138],\n",
       "        [-0.6719, -0.7149],\n",
       "        [-0.6711, -0.7157],\n",
       "        [-0.6731, -0.7137],\n",
       "        [-0.6691, -0.7177],\n",
       "        [-0.6718, -0.7149]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you don't have cuda installed, just remove the .cuda()\n",
    "dnn = MyDNN(numDistincWords)\n",
    "dnn = dnn.cuda()\n",
    "dnn(data.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What about training it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, loss:0.69353 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Batch: 280, loss:0.01337 "
     ]
    }
   ],
   "source": [
    "numEpochs = 20\n",
    "\n",
    "trainModel(numEpochs, dnn, cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nice training error, but what about validation??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3534, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(dnn, loss_function, cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a clear indication of overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1667102"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is overparametrized, we have two options:\n",
    "- reduce model complexity\n",
    "- add regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150.8012"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dnn.firstLayer.weight.data.cpu().numpy()**2) + \\\n",
    "    np.sum(dnn.firstLayer.weight.data.cpu().numpy()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $L^p$ regularization\n",
    "Is a technique to prevent weights in the nerual network to explode, by adding a penalization term in the loss function\n",
    "the new loss will be\n",
    "$loss = NLL + \\sum (|w_i|^p)^{\\frac{1}{p}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two famous lossess are with p=2 and p=1\n",
    "$L^2$ loss is already included in most of the optimization algorithms https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "For $L^1$ we need to go deeper in the code and insert it by hand as a penalization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpochWithPenalization(epochNum, model, loss_function, optimizer, factor, cuda=False):\n",
    "    for batchNum in range(numBatches):\n",
    "        # Step 1 PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get the datapoints and labels\n",
    "        dataBatch, labelsBatch = createMiniBatch(\n",
    "            corpus.train_positive, corpus.train_negative, batchSize, batchNum, numDistincWords)\n",
    "        if cuda:\n",
    "            dataBatch = dataBatch.cuda()\n",
    "            labelsBatch = labelsBatch.cuda()\n",
    "        # Step 3. Run our forward pass.\n",
    "        logProbs = model(dataBatch)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(logProbs, labelsBatch)\n",
    "        l1_crit = nn.L1Loss(size_average=False)\n",
    "        reg_loss = 0\n",
    "        for param in model.parameters():\n",
    "            target = torch.zeros(param.shape)\n",
    "            if cuda:\n",
    "                target = target.cuda()\n",
    "            reg_loss += l1_crit(param, target)\n",
    "\n",
    "        loss += factor * reg_loss\n",
    "        if batchNum % 20 == 0:\n",
    "            print('\\rEpoch: {0}, Batch: {1}, loss:{2:.5f}'.format(epochNum, batchNum, loss), flush=True, end=\" \")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "def trainModelWithPenalization(numEpochs, model, factor, cuda=False):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    for epochNum in range(numEpochs):\n",
    "        trainEpochWithPenalization(epochNum, model, loss_function, optimizer, factor, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, loss:0.79110 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Batch: 280, loss:0.10534  "
     ]
    }
   ],
   "source": [
    "dnn = MyDNN(numDistincWords)\n",
    "dnn = dnn.cuda()\n",
    "trainModelWithPenalization(20, dnn, 0.00001, useCuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.2332"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dnn.firstLayer.weight.data.cpu().numpy()**2) + \\\n",
    "    np.sum(dnn.firstLayer.weight.data.cpu().numpy()**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3531, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(dnn, loss_function, cuda=useCuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A different perspective: Dropout regularization\n",
    "http://jmlr.org/papers/v15/srivastava14a.html\n",
    "https://arxiv.org/pdf/1207.0580.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simply speaking, dropout sets randomly to zero some of the weights of the network ad each pass.\n",
    "The rationale behind it is that it should force the network to understand the nature of data without overfitting it. \n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html#dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Turn! use dropout to regularize the neural network, does it improve its ability to generalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDNNWithDropout(nn.Module):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0, Batch: 0, loss:0.69391 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Batch: 280, loss:0.14762 "
     ]
    }
   ],
   "source": [
    "dnn = MyDNNWithDropout(numDistincWords)\n",
    "trainModel(numEpochs, dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3526)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(dnn, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "1- write the code to perform real testing (e.g. classification accuracy) on the test set\n",
    "\n",
    "2- try to use torch.nn.sequential(...) to avoid having to write\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        out = self.firstLayer(bow_vec)\n",
    "        out = self.relu(out)\n",
    "        out = self.secondLayer(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.thirdLayer(out)\n",
    "        return F.log_softmax(out)\n",
    "\n",
    "3 - adapt all what has been done to work with sequences of digits insetad of words\n",
    "the problem you need to solve is the following: given a proper name, understand if it's masculine or feminine\n",
    "\n",
    "Does the model perform well? Can you explain why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
