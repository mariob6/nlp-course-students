{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's review last assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFolder = '../data/names/train/'\n",
    "validFolder = '../data/names/valid/'\n",
    "testFolder = '../data/names/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import CharCorpus\n",
    "corpus = CharCorpus(\n",
    "    trainFolder+'m.txt', trainFolder+'f.txt',\n",
    "    validFolder+'m.txt', validFolder+'f.txt',\n",
    "    testFolder+'m.txt', testFolder+'f.txt', limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.train_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numDistinctChars = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LRClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(LRClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LRClassifier(2, numDistinctChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMiniBatch(masculines, feminines, batch_size, batch_num, vecLen):\n",
    "    data = masculines[batch_num: batch_num + batch_size // 2]\n",
    "    data.extend(feminines[batch_num: batch_num + batch_size // 2])\n",
    "    data = list(map(lambda x: indicesToFeatures(x, vecLen), data))\n",
    "    data = torch.stack(data)\n",
    "    labels = [1] * (batch_size // 2) + [0] * (batch_size // 2)\n",
    "    labels = torch.tensor(labels)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def indicesToFeatures(seq, vecLen):\n",
    "    out = torch.zeros(vecLen)\n",
    "    for ind in seq:\n",
    "        out[ind] += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(epochNum, model, loss_function, optimizer, cuda=False):\n",
    "    for batchNum in range(numBatches):\n",
    "        # Step 1 PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get the datapoints and labels\n",
    "        dataBatch, labelsBatch = createMiniBatch(\n",
    "            corpus.train_1, corpus.train_2, batchSize, batchNum, numDistinctChars)\n",
    "        if cuda:\n",
    "            dataBatch = dataBatch.cuda()\n",
    "            labelsBatch = labelsBatch.cuda()\n",
    "        # Step 3. Run our forward pass.\n",
    "        logProbs = model(dataBatch)\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(logProbs, labelsBatch)\n",
    "        if batchNum % 20 == 0:\n",
    "            print('\\rEpoch: {0}, Batch: {1}, loss:{2:.5f}'.format(epochNum, batchNum, loss), flush=True, end=\" \")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def trainModel(numEpochs, model, cuda=False):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    for epochNum in range(numEpochs):\n",
    "        trainEpoch(epochNum, model, loss_function, optimizer, cuda) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "numEpochs = 10\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(clf.parameters(), lr=0.01)\n",
    "batchSize = 10\n",
    "numBatches = 300\n",
    "        \n",
    "trainModel(numEpochs, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.firstLayer = nn.Linear(input_dim, 200)\n",
    "        self.secondLayer = nn.Linear(200, 100)\n",
    "        self.thirdLayer = nn.Linear(100, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        out = self.firstLayer(bow_vec)\n",
    "        out = self.relu(out)\n",
    "        out = self.secondLayer(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.thirdLayer(out)\n",
    "        return F.log_softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = MyDNN(numDistinctChars)\n",
    "dnn = dnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 10\n",
    "\n",
    "trainModel(numEpochs, dnn, cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we missing out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/rnn_unfold.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Wikipedia:\n",
    "\n",
    "A recurrent neural network (RNN) is a class of artificial neural network where connections between nodes form a directed graph along a sequence. This allows it to exhibit temporal dynamic behavior for a time sequence. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X_i$: feature vector of size input_size\n",
    "$h_i$: hidden state vector of size hidden_size\n",
    "\n",
    "What happens inside cell A? It depends on the cell type, for \"vanilla RNN\":\n",
    "\n",
    "$$h_t = tanh(W_{hh} h_{t-1} + W_{xh} x_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vanilla_rnn_cell.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"output\" of an RNN is either the full sequence of hidden states or the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.variable import Variable\n",
    "\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, output_size, n_layers=1, verbose=False):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, n_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "\n",
    "        # input = B x S . size(0) = B\n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        # input:  B x S  -- (transpose) --> S x B\n",
    "        input = input.t()\n",
    "\n",
    "        # Embedding S x B -> S x B x I (embedding size)\n",
    "        if self.verbose:\n",
    "            print(\"\\t input\", input.size())\n",
    "        embedded = self.embedding(input)\n",
    "        if self.verbose:\n",
    "            print(\"\\t embedding\", embedded.size())\n",
    "\n",
    "        # Make a hidden\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        if self.verbose:\n",
    "            print(\"\\t RNN hidden output\", hidden.size())\n",
    "        hidden = hidden[0, :, :]\n",
    "        # Use the last layer output as FC's input\n",
    "        # No need to unpack, since we are going to use hidden\n",
    "        fc_output = self.fc(hidden)\n",
    "        if self.verbose:\n",
    "            print(\"\\t fc output\", fc_output.size())\n",
    "        return F.log_softmax(fc_output)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return Variable(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padToMaxLen(seq):\n",
    "    if seq.shape[0] > maxLen:\n",
    "        seq = seq[:maxLen]\n",
    "    zeros = torch.zeros(maxLen - seq.shape[0]).long()\n",
    "    return torch.cat((seq, zeros))\n",
    "\n",
    "def createMiniBatch(masculines, feminines, batch_size, batch_num, vecLen):\n",
    "    data = masculines[batch_num: batch_num + batch_size // 2]\n",
    "    data.extend(feminines[batch_num: batch_num + batch_size // 2])\n",
    "    data = list(map(padToMaxLen, data))\n",
    "    data = torch.stack(data).long()\n",
    "    labels = [1] * (batch_size // 2) + [0] * (batch_size // 2)\n",
    "    labels = torch.tensor(labels)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBatch, labelsBatch = createMiniBatch(\n",
    "        corpus.train_1, corpus.train_2, batchSize, 0, numDistinctChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RNNClassifier(numDistinctChars, 200, 200, 2)\n",
    "clf(dataBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModel(numEpochs, clf, cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go back to sentiment analysis, this time using chars as features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFolder = '../data/sentiment/train/'\n",
    "validFolder = '../data/sentiment/valid/'\n",
    "testFolder = '../data/sentiment/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = CharCorpus(\n",
    "    trainFolder+'positive.txt', trainFolder+'negative.txt',\n",
    "    validFolder+'positive.txt', validFolder+'negative.txt',\n",
    "    testFolder+'positive.txt', testFolder+'negative.txt', limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = 120\n",
    "numDistinctChars = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RNNClassifier(numDistinctChars, 100, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModel(numEpochs, clf, cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/vanishing_gradient.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla RNNs are not \"smart\" enough to backpropagate through time for long sequences, common problems are:\n",
    "    - vanishing gradients (loss does not improve)\n",
    "    - exploding gradients (loss goes to NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sepp Hochreiter; Jürgen Schmidhuber: Long-Short Memory Cells\n",
    "<img src=\"images/lstm_rnn_cell.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus joke: Look for #schmidubered\n",
    "\n",
    "<img src=\"images/schimdubered.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
